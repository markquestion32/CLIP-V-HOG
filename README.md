# CLIP-vs-HOG
comparaison between the two models
# Presentation of the two compared approaches
# 1.1 Introduction
Describing regions of interest in images is important to many computer vision tasks, including
object detection and content-based image retrieval (CBIR). Two distinct approaches for achieving this
are Histograms of Oriented Gradients (HOG) and Contrastive Language-Image Pretraining (CLIP).
While HOG is a traditional feature descriptor focusing on structural patterns, CLIP uses deep learning
to understand content. This section compares these two methods in terms of their principles.

# Histograms of Oriented Gradients (HOG)
The Histogram of Oriented Gradients (HOG) is a widely used feature descriptor in the fields of
computer vision and image processing. It examines the distribution of edge orientations within an
object to characterize its shape and visual characteristics. The HOG technique entails calculating the
gradient magnitude and orientation for every pixel in an image, followed by segmenting the image
into smaller cells.
![image](https://github.com/user-attachments/assets/359b79a2-b9d1-4fd7-8afa-2d338e15640e)


#Contrastive Language-Image Pretraining (CLIP)
CLIP is a deep learning model that aligns images and textual descriptions into a shared space,
enabling tasks like image description, retrieval, and classification.
This model can understand both text descriptions and images by utilizing a training method that fo-
cuses on contrasting pairs of text and images.
![image](https://github.com/user-attachments/assets/aa0b328b-3477-48be-8981-266e31b25e9a)

![image](https://github.com/user-attachments/assets/e5b27fa0-fb73-49a0-8580-06027075f90d)

